{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwlanyon/CardiacDTWorkshop/blob/main/Gaussian_Process_Emulators_For_Cardiac_Digital_Twins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtwti_lDZJwD"
      },
      "source": [
        "# Getting started with Gaussian process emulators\n",
        "\n",
        "This lab is designed to introduce Gaussian process emulation in a practical way using the Python package GPyTorch and the Modular Circ cardiac model.\n",
        "\n",
        "This lab session is intended to be introductory in nature due to the varied scientific backgrounds of the attendees. It involves visualising samples from Gaussian process priors, loading and normalising a datasets, learning a GP surrogate function for a cardiac digital twin and testing different GP kernel functions to assess emulator accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yrJ1GiFUoa3"
      },
      "outputs": [],
      "source": [
        "!pip install gpytorch # Installs the gpytorch package for generating Gaussian process models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLwHC5gFG-R9"
      },
      "outputs": [],
      "source": [
        "!pip install ModularCirc # Installs the ModularCirc package for running cardiac models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LsdofoDYQzc"
      },
      "outputs": [],
      "source": [
        "# Load important modules\n",
        "# Support for maths\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Plotting tools\n",
        "from matplotlib import pyplot as plt\n",
        "# we use the following for plotting figures in jupyter\n",
        "%matplotlib inline\n",
        "\n",
        "# For splitting training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For running the Modular Circ cardiac digital twin\n",
        "import ModularCirc\n",
        "\n",
        "#GpyTorch Gaussian Process library\n",
        "import gpytorch as gpt\n",
        "\n",
        "\n",
        "# Displaying images from URLs\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import urllib.request\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCd1i0nAuZB"
      },
      "source": [
        "The documentation for GPyTorch is avilable at https://docs.gpytorch.ai/en/stable/. We will be using GPyTorch to define our kernels, mean functions and GP models. Training the GP models is done externally using the ADAM optimiser in pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr6nu7bTREha"
      },
      "source": [
        "### Covariance functions, aka kernels\n",
        "\n",
        "We will define a covariance function, from hereon referred to as a kernel, using `GPyTorch`. The most commonly used kernel in machine learning is the Gaussian-form radial basis function (RBF) kernel. It is also commonly referred to as the exponentiated quadratic or squared exponential kernel &ndash; all are equivalent.\n",
        "\n",
        "The definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as:\n",
        "\n",
        "$$\n",
        "    \\kappa_\\mathrm{rbf}(x,x') = \\sigma^2\\exp\\left(-\\frac{(x-x')^2}{2\\mathscr{l}^2}\\right)\n",
        "$$\n",
        "\n",
        "It has two parameters, described as the variance, $\\sigma^2$ and the lengthscale $\\mathscr{l}$.\n",
        "\n",
        "In GPyTorch, we define our kernels using kernel modules. The basic RBF kernel does not include the variance parameter, $\\sigma^2$. To build a kernel with both lengthscale and variance parameters we need to wrap RBFKernel function inside of a ScaleKernel (this provides the variance parameter, $\\sigma^2$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laxqixGWA3B3"
      },
      "outputs": [],
      "source": [
        "# Here we define an RBF kernel with both lengthscale and variance parameters\n",
        "RBF_covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW2OL2wJ3WrL"
      },
      "source": [
        "This kernel has been instantiated with the default GPyTorch parameters (usually $\\ln(2)$), we can check them by calling them from the covariance module.\n",
        "\n",
        "Note that, because we generated our kernel by wrapping an unscaled RBF inside a scale kernel, we can call the variance or \"outputscale\" directly from the covariance module, but to call the lengthscale we need to access \"base_kernel\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9hQt2Dk2d_o"
      },
      "outputs": [],
      "source": [
        "RBF_covar_module.outputscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dv32Qe327Oh"
      },
      "outputs": [],
      "source": [
        "RBF_covar_module.base_kernel.lengthscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozelub4w4EDk"
      },
      "source": [
        "If we want to change the kernel variables we can set them ourselves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYFUayE04DrO"
      },
      "outputs": [],
      "source": [
        "RBF_covar_module.outputscale=1\n",
        "RBF_covar_module.base_kernel.lengthscale=1\n",
        "\n",
        "print(\"The variance is now\",RBF_covar_module.outputscale.tolist(),\"and the lengthscale is now\",RBF_covar_module.base_kernel.lengthscale.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsIiYD3vBZFs"
      },
      "source": [
        "Kernels can be combined using the \"+\" and \"*\" operators. Let's generate a scaled additive combination of the RBF and Matern 5/2 kernels. To do this we add a MaternKernel object and include the parameter nu=2.5 (for the 5/2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDEzTZ30BXyG"
      },
      "outputs": [],
      "source": [
        "Additive_covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel()+gpt.kernels.MaternKernel(nu=2.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiQuApBqB0Dv"
      },
      "source": [
        "To access the hyper-parameters we have to slightly change our code to account for the combined kernels. Below we access the Matern kernel lengthscale by calling the second kernel within the \"base_kernel\" object.\n",
        "\n",
        "Python indexing begins at 0, so to access the second kernel we use index 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRkVDu0FBz2E"
      },
      "outputs": [],
      "source": [
        "Additive_covar_module.base_kernel.kernels[1].lengthscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ_l8CihDx9T"
      },
      "source": [
        "For more information on all the different types of kernel and their input parameters see the documentation here: https://docs.gpytorch.ai/en/v1.12/kernels.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5rVKY4-xxlt"
      },
      "source": [
        "### Visualising the kernel\n",
        "\n",
        "We can visualise our kernel in a few different ways. We can plot the _shape_ of the kernel by plotting $k(x,0)$ over some sample space $x$ which, looking at the equation above, clearly has a Gaussian shape. This describes the covariance between each sample location and $0$.\n",
        "\n",
        "Alternatively, we can construct a full covariance matrix, $\\mathbf{K}_{xx} \\triangleq k(x,x')$ (with samples $x = x'$, generating a diagonal matrix). The resulting GP prior is a multivariate normal distribution over the space of samples $x$: $N(\\mathbf{0}, \\mathbf{K}_{xx})$. It should be evident then that the elements of the matrix represents the covariance between respective points in $x$ and $x'$, and that it is exactly $\\sigma^2[=1]$ in the diagonal.\n",
        "\n",
        "We can show this using `pyplot` to plot the vector $k(x,0)$ and the matrix $k(x,x')$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhr9sWaBx3bQ"
      },
      "outputs": [],
      "source": [
        "# Our sample space: 100 samples in the interval [-4,4]\n",
        "x1 = torch.linspace(-4.,4.,100)\n",
        "covar_1D = RBF_covar_module(x1,torch.tensor([0])) # Returns a RootLinearOperator, an object used to ease the computational burden of linear algebra\n",
        "\n",
        "# Gets the actual tensor for this kernel matrix using .to_dense()\n",
        "#we use detach().numpy() to remove the gradient and convert to a numpy array for plotting\n",
        "tensor_1D = covar_1D.to_dense().detach().numpy()\n",
        "\n",
        "covar_2D = RBF_covar_module(x1) # Returns a RootLinearOperator\n",
        "tensor_2D = covar_2D.to_dense().detach().numpy() # Gets the actual tensor for this kernel matrix\n",
        "\n",
        "# Set up the plotting environment\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "# ==== k(x,0)\n",
        "\n",
        "plt.subplot(121) # left plot\n",
        "\n",
        "# Plot covariance vector\n",
        "plt.plot(x1.detach().numpy(),tensor_1D)\n",
        "\n",
        "# Annotate plot\n",
        "plt.xlabel(\"x\"), plt.ylabel(\"$\\kappa$\")\n",
        "plt.title(\"$\\kappa_{rbf}(x,0)$\")\n",
        "\n",
        "# ==== k(x,x')\n",
        "\n",
        "plt.subplot(122) # right plot\n",
        "\n",
        "# Plot the covariance of the sample space\n",
        "plt.pcolor(x1.T.detach().numpy(), x1.detach().numpy(), tensor_2D)\n",
        "\n",
        "# Format and annotate plot\n",
        "plt.gca().invert_yaxis(), plt.gca().axis(\"image\")\n",
        "plt.xlabel(\"x\"), plt.ylabel(\"x'\"), plt.colorbar()\n",
        "plt.title(\"$\\kappa_{rbf}(x,x')$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axgG_pPnjBDv"
      },
      "source": [
        "We can also visualise how the kernel parameters affect the kernel function, below we vary the lengthscale, $l$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR99fqjbwbTS"
      },
      "outputs": [],
      "source": [
        "# Our sample space : 100 samples in the interval [-4,4]\n",
        "X = torch.linspace(-4.,4.,250) # we use more samples to get a smoother plot at low lengthscales\n",
        "\n",
        "# Create a 1-D RBF kernel with default parameters\n",
        "covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel())\n",
        "\n",
        "# Set up the plotting environment\n",
        "plt.figure(figsize=(18, 7))\n",
        "\n",
        "# Set up our list of different lengthscales\n",
        "ls = [0.25, 0.5, 1., 2., 4.]\n",
        "\n",
        "# Loop over the lengthscale values\n",
        "for l in ls:\n",
        "    # Set the lengthscale to be l\n",
        "    covar_module.base_kernel.lengthscale = l\n",
        "    # Calculate the new covariance function at k(x,0)\n",
        "    covar_1D = covar_module(X,torch.tensor([0])) # Returns a RootLinearOperator\n",
        "    C= covar_1D.to_dense().detach().numpy()\n",
        "    # Plot the resulting covariance vector\n",
        "    plt.plot(X,C)\n",
        "\n",
        "# Annotate plot\n",
        "plt.xlabel(\"x\"), plt.ylabel(\"$\\kappa(x,0)$\")\n",
        "plt.title(\"Effects of different lengthscales on the Gaussian RBF kernel\")\n",
        "plt.legend(labels=ls);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS5oVNAe7lnP"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "(a) What is the effect of the lengthscale parameter on the covariance function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFHtT9L07oZ5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRZXqqPv7qvj"
      },
      "source": [
        "(b) Change the code used above to plot the covariance function showing the effects of the variance on the covariance function. Comment on the effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RMotnYG7gqf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62XQWHtVjXP4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxeVkoDqB_F8"
      },
      "source": [
        "## 2. Types of covariance function\n",
        "\n",
        "There are many different covariance functions already implemented in `GPy`. Aside from the `RBF` kernel, there are others such as the following:\n",
        "- `Cosine`\n",
        "- `Matern`\n",
        "- `Polynomial`\n",
        "- `Linear`\n",
        "\n",
        "You can also combine kernels using the `additive` and `structure` kernels. Adding and multiplying kernels can be acheived simply by using the \"+\" and \"*\" operators.\n",
        "\n",
        "See the docs here: https://docs.gpytorch.ai/en/v1.14/kernels.html\n",
        "\n",
        "Note: when defining these, all are preceded by `gpt.kernels.` The following are some examples of the [Matérn 5/2](https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function) and Cosine kernels, compared with the RBF kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNJgXJtvB-3L"
      },
      "outputs": [],
      "source": [
        "# Our sample space : 100 samples in the interval [-4,4]\n",
        "X = torch.linspace(-4.,4.,250)\n",
        "\n",
        "# RBF kernel\n",
        "k_R = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel())\n",
        "C_R = k_R(X, torch.tensor([[0.]])).to_dense().detach().numpy()\n",
        "\n",
        "# Matern 5/2\n",
        "k_M = gpt.kernels.ScaleKernel(gpt.kernels.MaternKernel(nu=2.5))\n",
        "C_M = k_M(X, torch.tensor([[0.]])).to_dense().detach().numpy()\n",
        "\n",
        "# Cosine\n",
        "k_C = gpt.kernels.ScaleKernel(gpt.kernels.CosineKernel())\n",
        "C_C = k_C(X, torch.tensor([[0.]])).to_dense().detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(18,7))\n",
        "plt.plot(X, C_R, X, C_M, X, C_C);\n",
        "plt.xlabel(\"x\"), plt.ylabel(\"$\\kappa$\")\n",
        "plt.legend(labels=[\"Gaussian RBF\", \"Matérn 5/2\", \"Cosine\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fleOZ_u4HvjG"
      },
      "source": [
        "Not every kernel has the same set of parameters. Some kernels are not parameterised by a lengthscale, for example, like the `Linear` kernel which only has a list of variances corresponding to each linear component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tuBFVA7FwwW"
      },
      "outputs": [],
      "source": [
        "L=gpt.kernels.LinearKernel(ard_num_dims=3) #Generate a linear kernel with three input dimensions\n",
        "L.variance #show the variances of each linear component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Hy41hDJOe7"
      },
      "source": [
        "Likewise, not every kernel is stationary. In the case of the Gaussian RBF, or Matérn kernels, the kernel can be written $\\kappa(x,x') = f(x-x')$, however this is not true for, e.g., the polynomial covariance function, which is defined as $k(x,x') = (x^\\top x'+c)^d$. This means that the covariance between points depends on the input points themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eH1Key7Hft-"
      },
      "outputs": [],
      "source": [
        "# Our sample space : 100 samples in the interval [-5,5]\n",
        "X = torch.linspace(-5., 5., 100)[:,None]\n",
        "\n",
        "# Note that the Polynomial kernel is defined:\n",
        "#   k(x,x') = (xTx'+c)^d\n",
        "\n",
        "# We define our polynomial kernel\n",
        "k_b = gpt.kernels.ScaleKernel(gpt.kernels.PolynomialKernel(power=2))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18,7))\n",
        "\n",
        "x_s = [0., .5, 1., 2.,3.] # values of x'\n",
        "# Loop through values of x'\n",
        "for x_ in x_s:\n",
        "    # Evaluate kernel at k(x,x')\n",
        "    K_B = k_b(X, torch.tensor([[x_]])).to_dense().detach().numpy()\n",
        "    # Plot covariance vector\n",
        "    plt.plot(X, K_B)\n",
        "\n",
        "# Annotate plot\n",
        "plt.xlabel(\"x\"), plt.ylabel(\"$\\kappa$\")\n",
        "plt.title(\"Effects of different inputs on a non-stationary, Polynomial kernel\")\n",
        "plt.legend(labels=[\"$\\kappa(x,0)$\", \"$\\kappa(x,0.5)$\", \"$\\kappa(x,1)$\", \"$\\kappa(x,2)$\", \"$\\kappa(x,3)$\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaKK2aS6kp6O"
      },
      "source": [
        "How might we interpret the covariance structure indicated in the above plot? When might the polynomial kernel be useful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvdlBwRXkppu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGk3VHesrRPt"
      },
      "source": [
        "# Sampling from GP priors\n",
        "\n",
        "As previously discussed the covariance function determines the prior Gaussian process, let's sample from a prior GP to show how the covariance affects the resulting function space.\n",
        "\n",
        "To do this we'll need to create a python class with the appropriate mean and covariance functions. You can see more info in the basic GPyTorch regression tutorial: https://docs.gpytorch.ai/en/v1.12/examples/01_Exact_GPs/Simple_GP_Regression.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snoqtfjQp_dY"
      },
      "outputs": [],
      "source": [
        "# To sample from the GP prior with GPyTorch we need to define a basic GP class\n",
        "#We will use the simplest form of GP model, exact inference (as opposed to something like sparse GPs)\n",
        "# The model is defined by mean and covariance modules\n",
        "\n",
        "class ExactGPModelRBF(gpt.models.ExactGP): # Define a class of GPs using the GPyTorch exact GP class\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModelRBF, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpt.means.ZeroMean(input_size=1) # Define the mean module\n",
        "        self.covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel()) # Define the covariance module\n",
        "\n",
        "    def forward(self, x):  # Function to call the mean and covariance when evaluating the model\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpt.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# initialize likelihood and model\n",
        "likelihood = gpt.likelihoods.GaussianLikelihood() # Define a gaussian likelihood\n",
        "#GPyTorch automatically conditions on training data so here we generate a model with empty tensors\n",
        "model = ExactGPModelRBF(torch.tensor([]), torch.tensor([]), likelihood) # Generate the model with no training data\n",
        "model.covar_module.base_kernel.lengthscale = 1 # Set the RBF kernel lengthscale to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPBUozcWJH8W"
      },
      "source": [
        "GPyTorch models and likelihoods have two modes, training and evaluation, you can change between them by using \".train()\" and \".eval()\".\n",
        "\n",
        "In the training mode you can perform optimisation of the model and likelihood hyperparameters but cannot make predictions or sample from the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2m_AhMxqCh5"
      },
      "outputs": [],
      "source": [
        "# Set the model and likelihood to evaluate mode\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "# Plot samples from the prior\n",
        "x0=torch.linspace(-5,5,100) # parameter space\n",
        "plt.plot(x0,model(x0).sample(sample_shape=torch.Size([10],)).T); # plot samples from the GP prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_9iLdtbMXO1"
      },
      "source": [
        "# Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM_GX23wroeA"
      },
      "source": [
        "Above we set the kernel lengthscale to 1. What happens if we make the lengthscale larger? Generate samples from a GP with lengthscale=3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFxkljYGrE50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqOpIaGIssGD"
      },
      "source": [
        "Or shorter? Generate samples from a GP with lengthscale=0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGXDBmRHstf4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufqmg8MbNXqp"
      },
      "source": [
        "What about if we change the kernel variance? Print the current kernel variance and then generate samples from a GP with an increased variance. Does it behave as you'd expect based on your results from exercise 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd8lbs_VNyff"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYFApfjLquUK"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "Re-write the code above to sample from GP priors with different covariance functions and visualise the results. In particular, try combining covariance functions and changing the kernel parameters.\n",
        "\n",
        "For example: Combine a linear kernel with a cosine kernel.\n",
        "\n",
        "NB: You'll need to create a new GP class with a new covar_module. If you rename the class remember to also change the name in the super argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mz93l_Vq9RO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWTPiF_jmXjB"
      },
      "source": [
        "A nice tool for visualising different kernels: https://www.infinitecuriosity.org/vizgp/\n",
        "\n",
        "Changing the kernel alters the GP prior and you can place points to see the resulting predicted function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usypOaO6LpLw"
      },
      "source": [
        "## 3. 1D Gaussian process regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLjJ7xeDMMob"
      },
      "source": [
        "We've looked at Gaussian process priors and covariance functions. Let's try some Gaussian process regression!\n",
        "\n",
        "Here we will generate data from a toy function and use it to train a GP emulator.\n",
        "\n",
        "# Example 1: 1D input, 1D output\n",
        "\n",
        "Creating an emulator for the function $$f(\\theta)=sin(4\\theta)+\\theta$$ from noisy observations.\n",
        "\n",
        "$$\\theta \\in [0,1]$$\n",
        "\n",
        "Our observational model is $$y_i=f(\\theta_i)=g(\\theta_i)+\\epsilon_i$$ where $$\\epsilon_i \\sim N(0,\\sigma_o^2).$$\n",
        "\n",
        "We aim to train a gaussian process emulator, $g$, such that $$g(\\theta) \\approx f(\\theta)$$\n",
        "\n",
        "The hyper-parameters we seek to learn are the GP hyper-parameters and the observation noise variance, $\\sigma_o^2$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veWgmOphQu9Y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfSfD5B-G_OE"
      },
      "outputs": [],
      "source": [
        "def oneD_func(theta): # define f(theta)\n",
        "    return torch.sin(4*theta) + theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuZgkpcfLpt5"
      },
      "outputs": [],
      "source": [
        "# Set observation upper and lower bounds\n",
        "lb= -1. # lower bound\n",
        "ub = 1. # upper bound\n",
        "\n",
        "p = 5 # Number of observations (more means better emulation)\n",
        "\n",
        "p2=50\n",
        "\n",
        "theta_train = torch.linspace(lb,ub,p) # Generate training inputs\n",
        "\n",
        "y_train=oneD_func(theta_train)\n",
        "\n",
        "theta_test = torch.linspace(lb,ub,p2) # Generate test inputs\n",
        "\n",
        "y_test= oneD_func(theta_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVy0c01aXaSs"
      },
      "source": [
        "Let's define a GP model with GPyTorch. We'll use an RBF kernel and a zero mean function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vatpAxGsXZ7x"
      },
      "outputs": [],
      "source": [
        "# We will use the simplest form of GP model, exact inference\n",
        "class ExactGPModel(gpt.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpt.means.ZeroMean(input_size=1)\n",
        "        self.covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpt.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# initialize likelihood and model\n",
        "likelihood = gpt.likelihoods.GaussianLikelihood()\n",
        "model = ExactGPModel(theta_train, y_train, likelihood) #By including the training data we generate a posterior GP\n",
        "\n",
        "likelihood.eval()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBs_1_rLX-qT"
      },
      "outputs": [],
      "source": [
        "# Train-test split randomises the data\n",
        "#here we sort the test data back from min -> max\n",
        "theta_test=theta_test.sort()[0]\n",
        "\n",
        "# Predict\n",
        "predictions_untrained=likelihood(model(theta_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQr02-fVeOb"
      },
      "source": [
        "We can plot our predictions and uncertainty against the real function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLAFEQTK1lSi"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "\n",
        "    # Get upper and lower confidence bounds\n",
        "    lower, upper = predictions_untrained.confidence_region()\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "    # Plot predictive means as blue line\n",
        "    ax.plot(theta_test, predictions_untrained.mean.numpy(), 'b')\n",
        "    # Shade between the lower and upper confidence bounds\n",
        "    ax.fill_between(theta_test, lower.numpy(), upper.numpy(), alpha=0.5)\n",
        "    #ax.set_ylim([0, 3])\n",
        "\n",
        "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rss7ZBr5VjlP"
      },
      "source": [
        "We can also sample from the GP posterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q94KSoSq_h-S"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "    #ax.set_ylim([0, 3])\n",
        "    ax.plot(theta_test,model(theta_test).sample(sample_shape=torch.Size([100],)).T);\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "\n",
        "    ax.legend(['Observed Data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_9QGPoOVa3J"
      },
      "source": [
        "## GP posterior recap\n",
        "\n",
        "Recall that our prior over observations and targets is jointly Gaussian, using the rule for Gaussian conditionals, the posterior distribution over a set of test points, $$\\pi(g(X_*)|y)\\sim N(\\mu_p,\\Sigma_p)$$\n",
        "where\n",
        "$$\\mu_p =  K(X_*,X)(K(X_*,X)+\\sigma_o^2)^{-1}y$$\n",
        "and\n",
        "$$K_p = K(X_*,X_*)-K(X_*,X)(K(X,X)+\\sigma_o^2)^{-1}K(X,X_*)$$\n",
        "\n",
        "(given that we start with a zero-mean GP).\n",
        "\n",
        "Though we are yet to optimise any hyperparameters, because GPyTorch automatically generates this posterior, you can see that the GP has already begun to approximate the true function (as opposed to simply sampling from the prior).\n",
        "\n",
        "However, though our GP posterior mean is approximating the shape of the observed data, our predictions aren't very good. Let's optimise the GP hyper-parameters and the observation noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AhQubVMZfqv"
      },
      "outputs": [],
      "source": [
        "# Find optimal model hyperparameters\n",
        "\n",
        "\n",
        "# For ease, define a function to train the emulators using the adam optimiser\n",
        "\n",
        "def train_model(model,likelihood,y_train,theta_train,training_iter=500):\n",
        "  # Set model and likelihood to train mode\n",
        "  model.train()\n",
        "  likelihood.train()\n",
        "  # Use the adam optimiser\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
        "\n",
        "  # \"Loss\" for GPs - the marginal log likelihood\n",
        "  mll = gpt.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "  for i in range(training_iter):\n",
        "      # Zero gradients from previous iteration\n",
        "      optimizer.zero_grad()\n",
        "      # Output from model\n",
        "      output = model(theta_train)\n",
        "      # Calc loss and backprop gradients\n",
        "      loss = -mll(output, y_train)\n",
        "      loss.backward() # generate gradient\n",
        "      optimizer.step()\n",
        "\n",
        "train_model(model,likelihood,y_train,theta_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQihGShgm6j"
      },
      "source": [
        "Let's take a look at predictions from our optimised model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S3eHI5p2aLu"
      },
      "outputs": [],
      "source": [
        "model.eval() # Set model to evaluate mode\n",
        "likelihood.eval() # Set likelihood to evaluate mode\n",
        "predictions_trained=likelihood(model(theta_test)) # Generate predictions over the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQxgHk0qZ3PF"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "\n",
        "    # Get upper and lower confidence bounds\n",
        "    lower, upper = predictions_trained.confidence_region()\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "    # Plot predictive means as blue line\n",
        "    ax.plot(theta_test, predictions_trained.mean.numpy(), 'b')\n",
        "    # Shade between the lower and upper confidence bounds\n",
        "    ax.fill_between(theta_test, lower.numpy(), upper.numpy(), alpha=0.5)\n",
        "    #ax.set_ylim([0, 3])\n",
        "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_qRoBq7AZ-g"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "    #Sample from posterior (without likelihood)\n",
        "    ax.plot(theta_test,model(theta_test).sample(sample_shape=torch.Size([100],)).T);\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "\n",
        "    ax.legend(['Observed Data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RPIiWTvLR2t"
      },
      "source": [
        "What happens outside of our training range?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED9nI5aLjY-c"
      },
      "outputs": [],
      "source": [
        "# Let's define a test set over a wider interval\n",
        "q=200\n",
        "theta2 = torch.linspace(-2*ub,2*ub,q) # Generate theta={theta_i}\n",
        "# Predict over the wider interval\n",
        "predictions_extended=likelihood(model(theta2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLe35RZy04QK"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "\n",
        "    # Get upper and lower confidence bounds\n",
        "    lower, upper = predictions_extended.confidence_region()\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta2,oneD_func(theta2))\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "    # Plot predictive means as blue line\n",
        "    ax.plot(theta2, predictions_extended.mean.numpy(), 'b')\n",
        "    # Shade between the lower and upper confidence bounds\n",
        "    ax.fill_between(theta2, lower.numpy(), upper.numpy(), alpha=0.5)\n",
        "    #ax.set_ylim([0, 3])\n",
        "    ax.legend(['True function','observed data', 'Mean', 'Confidence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nX3XstYAd02"
      },
      "outputs": [],
      "source": [
        "# Sample from the posterior\n",
        "with torch.no_grad():\n",
        "    # Initialize plot\n",
        "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
        "    # Sample from the posterior (without likelihood)\n",
        "    ax.plot(theta2,model(theta2).sample(sample_shape=torch.Size([100],)).T);\n",
        "    # Plot training data as black stars\n",
        "    ax.plot(theta_train,y_train, 'k*')\n",
        "\n",
        "    ax.legend(['Observed Data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5x9ONjRhqd1"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "Train the model using more training points (within the original parameter range) and plot the mean prediction and uncertainty over the extended range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaPRifsuh1qX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PsbMx9Rh2IW"
      },
      "source": [
        "What effect does that have on the uncertainty?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyS-12j_h5yc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hRyup7h5Z3"
      },
      "source": [
        "Does it improve our predictions outside of the training range at all?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb8pzyyniCrG"
      },
      "source": [
        "Given our knowledge of the function, change the model's kernel function (by addition or multiplication of kernels) to try and increase the extrapolative capability of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r72eVzcJiQZ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iEjiDP9iRYS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AApDxOV_NdD5"
      },
      "source": [
        "# Emulating a cardiac digital twin\n",
        "\n",
        "In this section we'll put generate simulations from the Naghavi et al. model of left ventricular contractility using the Modular Circ python library. We'll then train a Gaussian process emulator of the model and use it to perform global sensitivity analysis.\n",
        "\n",
        "The Naghavi model is a 0D lumped parameter model of the circulatory system described in Rapid Estimation of Left Ventricular Contractility with a Physics-Informed Neural Network Inverse Modeling Approach (https://arxiv.org/html/2401.07331v1). The model is comprised of the following components:\n",
        "\n",
        "LA: linear time-varying elastance model (6 parameters)\n",
        "MV: non-ideal diode model (1 parameter)\n",
        "LV: linear time-varying elastance model (5 parameters)\n",
        "AV: non-ideal diode model (1 parameter)\n",
        "Aorta: RC Windkessel model (2 parameters)\n",
        "Vena cava: RC Windkessel model (2 parameters)\n",
        "Total set of parameters sums up to 17.\n",
        "\n",
        "The model can be expressed using a circuit diagram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydhranHFl6YE"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://arxiv.org/html/2401.07331v1/extracted/5346203/Circuit.png'\n",
        "with urllib.request.urlopen(url) as url:\n",
        "    img = Image.open(BytesIO(url.read()))\n",
        "\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_LPAXcgNJUc"
      },
      "source": [
        "To run the model using ModularCirc we need to load a json file that includes the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zQ9I4IW8UGO"
      },
      "outputs": [],
      "source": [
        "#Load the file from github\n",
        "!wget https://raw.githubusercontent.com/cwlanyon/CardiacDTWorkshop/refs/heads/main/parameters_01.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMM7kvMVi4F6"
      },
      "source": [
        "To view the file click on the file button on the left. For each parameter you will find a reference value and for those parameters that are varying there is a proportional range within which to vary them.\n",
        "\n",
        "In this case were are only varying 6 parameters of a possible 17. One might interpret this as \"personalising\" the model for a patient with some known parameter values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iue0AIAjNr11"
      },
      "source": [
        "Next we import the Naghavi model and the batch runner function. This will allow us to sample from the parameter space and run the model for each sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpJulVc6KGcg"
      },
      "outputs": [],
      "source": [
        "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters, TEMPLATE_TIME_SETUP_DICT #Import functions to run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wapY8cVKMFB"
      },
      "outputs": [],
      "source": [
        "from ModularCirc import BatchRunner # Import the batch runner to run for multiple parameter inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vQAKls2NMYS"
      },
      "outputs": [],
      "source": [
        "br = BatchRunner('LHS', 0) # Initialise the batch runner with latin-hypercube sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCZvouUhNSZ7"
      },
      "outputs": [],
      "source": [
        "br.setup_sampler('parameters_01.json') #Set up the sampler with our parameters from the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqro9HnpNV22"
      },
      "outputs": [],
      "source": [
        "br.sample(100) # Take 100 samples from the parameter space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mChz3LehNW5p"
      },
      "outputs": [],
      "source": [
        "br.samples #view the samples, the first six are the varying parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCgaSMOBwtPj"
      },
      "outputs": [],
      "source": [
        "br.setup_model(model=NaghaviModel, po=NaghaviModelParameters, time_setup=TEMPLATE_TIME_SETUP_DICT) # Initialise the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppCd7ibXkDeB"
      },
      "outputs": [],
      "source": [
        "# Rescale the timing parameters and map to actual parameters where appropriate.\n",
        "map_ = {\n",
        "    'lv.t_tr' : ['lv.t_tr',],\n",
        "    'la.t_tr' : ['la.t_tr',],\n",
        "    'la.delay' : ['la.delay',],\n",
        "    'lv.tau' : ['lv.tau',],\n",
        "    'la.tau' : ['la.tau',],\n",
        "    'lv.t_max' : ['lv.t_max',],\n",
        "    'la.t_max' : ['la.t_max',],\n",
        "}\n",
        "br.map_sample_timings(\n",
        "    ref_time=1000.,\n",
        "    map=map_\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajEScJqMkJjY"
      },
      "outputs": [],
      "source": [
        "#use map_vessel_volumes to define the initial distribution of blood in the vessels\n",
        "br.map_vessel_volume()\n",
        "br._samples[['ao.v', 'art.v', 'ven.v']].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kex6ld55wyjC"
      },
      "outputs": [],
      "source": [
        "# Set up path for output files\n",
        "import os\n",
        "path = os.getcwd()\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkG7qxq4w3Gs"
      },
      "outputs": [],
      "source": [
        "# Run the models for our batched parameters (this will take a couple of minutes!)\n",
        "os.system(f'mkdir -p {path+ \"/Outputs/Out_01\"}')\n",
        "all_out = br.run_batch(n_jobs=2, output_path=path+'/Outputs/Out_01') # Creates a list object of the outputs for each run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHduqaHCw9i0"
      },
      "outputs": [],
      "source": [
        "# We can plot the volume transients for one of the simulations:\n",
        "\n",
        "ind=0 # set an index\n",
        "\n",
        "t = all_out[ind].loc[ind]['T'] - all_out[ind].loc[ind]['T'].loc[0] # extract the simulation times\n",
        "\n",
        "fig, ax = plt.subplots() # initalise the plotting environmemt\n",
        "\n",
        "ax.plot(t, all_out[ind].loc[ind]['v_lv'], label='lv') # plot the left ventricular volume transient\n",
        "ax.plot(t, all_out[ind].loc[ind]['v_la'], label='la') # plot the left atrial volume transient\n",
        "\n",
        "ax.set_xlabel('ms') # label axes\n",
        "ax.set_ylabel('ml')\n",
        "\n",
        "ax.legend() # generate legend\n",
        "ax.set_title('Volume transients')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PURlmmOckngI"
      },
      "source": [
        "You can view the outputs in the Out_01 file inside of the Outputs file. Each file contains the pressure, flow and volume transients at each time point of the simulation for each of the heart regions in the model.\n",
        "\n",
        "In this scenario we want to generate summary statistics that tell us something about the model across the whole time range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV8sPAs4w45E"
      },
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "max_values = torch.zeros(len(all_out),all_out[0].shape[1])\n",
        "min_values = torch.zeros(len(all_out),all_out[0].shape[1])\n",
        "for i in range(len(all_out)): # Loop over each item in all_out\n",
        "  max_values[i]=torch.tensor(all_out[i].max(axis=0).values)\n",
        "  min_values[i]=torch.tensor(all_out[i].min(axis=0).values)\n",
        "\n",
        "diff = max_values-min_values # Calculates max difference between min and max output values\n",
        "\n",
        "theta = torch.tensor(br.samples.values)[:,0:6] # Select only the varying input parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVDoXseQBg44"
      },
      "source": [
        "We've got lots of different outputs we could choose from. In the table below \"v\" refers to volume, \"p\" to pressure and \"q\" to flow rates. The suffixes after the underscore are the left ventricle (lv), aorta (ao), peripheral arteries (art), vena cava (vc), and left atrium (la)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iZoATxgBk5T"
      },
      "outputs": [],
      "source": [
        "# Output summary from the first run of the batch runner\n",
        "ind = 0\n",
        "all_out[ind].loc[ind].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSq8xVP5BOs2"
      },
      "source": [
        "In this case we're going to emulate the max aortic volume difference over the timescale, diff[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiTt7ywSmp06"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split into training and desting data\n",
        "theta_train, theta_test, y_train, y_test = train_test_split(\n",
        "    theta, #inputs\n",
        "    diff[:,0], #outputs\n",
        "    test_size=0.2, # proportion (or integer valued) of total data set used for testing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxJn0-DMn953"
      },
      "source": [
        "To improve training stability and prediction it is often useful to normalise both the input and output data for GPs. Here we normalise everything relative to the training data. This is because, in general, we should treat test data as completely unseen and normalising the entire dataset before the train test split might bias our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HOI4U5mmgeY"
      },
      "outputs": [],
      "source": [
        "# Calculate input mean and standard deviation\n",
        "y_train_mean = y_train.mean(axis=0)\n",
        "y_train_std = y_train.std(axis=0)\n",
        "\n",
        "# Calculate output mean and standard deviation\n",
        "theta_train_mean = theta_train.mean(axis=0)\n",
        "theta_train_std = theta_train.std(axis=0)\n",
        "\n",
        "# Scale training data\n",
        "theta_train = (theta_train - theta_train_mean)/theta_train_std\n",
        "y_train = (y_train - y_train_mean)/y_train_std\n",
        "\n",
        "# Scale testing data inputs\n",
        "theta_test = (theta_test - theta_train_mean)/theta_train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfLh595Qnrdv"
      },
      "outputs": [],
      "source": [
        "# Once again we initialise a GP, this time with 6 input parameters\n",
        "# We use the ard_num_dims argument to vary the lengthscale for each parameter\n",
        "class ExactGPModel(gpt.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpt.means.ZeroMean(input_size=6)\n",
        "        self.covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel(ard_num_dims=6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpt.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# initialize likelihood and model\n",
        "likelihood = gpt.likelihoods.GaussianLikelihood()\n",
        "model = ExactGPModel(theta_train, y_train, likelihood)\n",
        "\n",
        "likelihood.train()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOU8jLkvoq98"
      },
      "source": [
        "Then we train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG5CkRIToNyb"
      },
      "outputs": [],
      "source": [
        "train_model(model,likelihood,y_train,theta_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcwbGvsCo8pN"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluate mode\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pEiMgQno2A2"
      },
      "source": [
        "Let's make and evaluate some predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iznme3jRoR4l"
      },
      "outputs": [],
      "source": [
        "# Mean prediction\n",
        "mean_pred=likelihood(model(theta_test)).mean\n",
        "print(mean_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azrf3hJGo64x"
      },
      "outputs": [],
      "source": [
        "# True value\n",
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjFBC_icpMoP"
      },
      "source": [
        "These don't look very similar, but remember, we need to rescale the output of our GP to match the real world scale of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0TpNB6pMVM"
      },
      "outputs": [],
      "source": [
        "# Rescale the predictions\n",
        "\n",
        "mean_pred = mean_pred*y_train_std + y_train_mean\n",
        "print(mean_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Hj33a2pgWT"
      },
      "source": [
        "That's a bit more like it!\n",
        "\n",
        "We can evaluate the quality of our emulator more rigorously using the mean squared error and the $R^2$.\n",
        "\n",
        "For observations $y$ and predictions $y^*$\n",
        "\n",
        "$$ MSE = \\frac{1}{N}\\Sigma_n^N(y_n^*-y_n)^2$$\n",
        "\n",
        "\n",
        "$$ R^2 = 1- \\frac{\\Sigma_n^N(y_n^*-y_n)^2}{\\Sigma_n^N(y_n-\\bar{y_n})^2}$$\n",
        "\n",
        "We'd like MSE to be small (relative to the scale of the data) and $R^2$ to be close to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gheEMekVpt29"
      },
      "outputs": [],
      "source": [
        "# Define MSE and R^2 functions\n",
        "\n",
        "def MSE(pred,obs):\n",
        "  mse = ((pred-obs)**2).mean(axis=0)\n",
        "  return mse\n",
        "\n",
        "def R2(pred,obs):\n",
        "  r2=1-MSE(pred,obs)/obs.var(axis=0)\n",
        "  return r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeii1OLKpEMJ"
      },
      "outputs": [],
      "source": [
        "MSE(mean_pred,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe5SXSPSxzze"
      },
      "outputs": [],
      "source": [
        "R2(mean_pred,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0i2U7Jl8tFC"
      },
      "source": [
        "That's pretty good! let's save this model for use later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAjSJ2y48wLX"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_state.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSNa0Wy76P0C"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "Try emulating the data with different mean and covariance functions, can we improve the $R^2$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BWZKBYh6cmV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zyEDYb-Da6K"
      },
      "source": [
        "# 4. Global Sensitivity Analysis (GSA)\n",
        "\n",
        "GSA is a method for determining which inputs a model is most sensitive to. It is generally a high throughput method that requires many samples from parameter space. In cases where models take a long time to run, emulators are a good fit for performing GSA as they can often evaluate the forward problem very quickly.\n",
        "\n",
        "Today we'll use the Sobol Indices method of GSA: https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis\n",
        "\n",
        "Sobol indices indicate how senstive an output is to its various inputs, based on the output variance as the inputs vary. The higher the Sobol indice, the more sensitive the model is to that parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIupCcUOG8oK"
      },
      "outputs": [],
      "source": [
        "# Install the SALib library to perform GSA\n",
        "!pip install SALib;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzUvtN53DfTu"
      },
      "outputs": [],
      "source": [
        "#Import libraries for sensitivity analysis\n",
        "\n",
        "import scipy.stats.qmc as qmc\n",
        "##### from SALib.sample import saltelli\n",
        "from SALib.sample import saltelli\n",
        "from SALib.analyze import sobol\n",
        "from SALib.test_functions import Ishigami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIrPBbJv9CgN"
      },
      "source": [
        "To perform the GSA, let's load the model we saved earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjjBzefW9Biq"
      },
      "outputs": [],
      "source": [
        "# We need to load the model state dictionary and reinstate the model\n",
        "# Note that we need include the training data in the loaded model\n",
        "# This is because GPyTorch conditions the posterior based on the training data\n",
        "\n",
        "# We initialise a GP class matching our saved GP\n",
        "class ExactGPModel(gpt.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpt.means.ZeroMean(input_size=6)\n",
        "        self.covar_module = gpt.kernels.ScaleKernel(gpt.kernels.RBFKernel(ard_num_dims=6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpt.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "state_dict = torch.load('model_state.pth')\n",
        "model = ExactGPModel(theta_train,y_train, likelihood)  # Create a new GP model\n",
        "\n",
        "model.load_state_dict(state_dict) # Update the new model's parameters to the saved parameters\n",
        "\n",
        "# Put the model and likelihood into eval mode\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEskx96r_4QD"
      },
      "source": [
        "Next we can run the sensitivity analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIeWy7jayTkQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "nDim = theta_train.shape[1] # Number of input parameters\n",
        "boundsMaxMin = [] # Initialise a list for the max-min bounds of the parameters\n",
        "for i in range(nDim): # Calculates the max and min bounds of the parameters\n",
        "    boundsMaxMin.append([np.min(theta_train[:,i].detach().numpy()),np.max(theta_train[:,i].detach().numpy())])\n",
        "\n",
        "# Define the model inputs\n",
        "problem = {\n",
        "    'num_vars': nDim, # Number of parameters\n",
        "    'names': ['ao.c',\t'art.r',\t'art.c',\t'ven.c',\t'lv.E_act',\t'lv.k_pas'], # Parameter names\n",
        "    'bounds': boundsMaxMin # Bounds for each parameter\n",
        "}\n",
        "\n",
        "# Generate samples via a saltelli sample\n",
        "param_values = saltelli.sample(problem, 1024)\n",
        "\n",
        "data1 = torch.tensor(param_values) #convert to torch for our emulator\n",
        "\n",
        "#use the emulator to predict the function at the sampled parameter values and rescale the prediction\n",
        "\n",
        "Ymean=y_train_mean*likelihood(model((data1))).mean+y_train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T8vnbSzKkLd"
      },
      "outputs": [],
      "source": [
        "# Analyse the sobol indices and save the first and total effects\n",
        "\n",
        "Si=sobol.analyze(problem, Ymean.detach().numpy(), print_to_console=True) # Calculate Sobol indices\n",
        "\n",
        "total_Si, first_Si, second_Si = Si.to_df()\n",
        "Si_first =  first_Si.iloc[:,0]\n",
        "Si_total =  total_Si.iloc[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w02RQ4KSLMm9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "inputs =  ['ao.c',\t'art.r',\t'art.c',\t'ven.c',\t'lv.E_act',\t'lv.k_pas']\n",
        "outputs = ['ao_vol_diff']\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(Si_first.values[:,None],cmap='magma_r',vmin=0,vmax=1)\n",
        "\n",
        "ax.set_xticks(np.arange(len(outputs)),labels=outputs)\n",
        "ax.set_yticks(np.arange(len(inputs)), labels=inputs)\n",
        "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
        "         rotation_mode=\"anchor\");\n",
        "\n",
        "plt.colorbar(im,fraction=0.015, pad=0.04)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFKsXBQ9SI3"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "Train a Gaussian process emulator on the total ventricular volume difference (diff[:,2]), test it's accuracy and perform GSA. Do the most important inputs change? Do they change as you'd expect them to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mcj8AsJMSLH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuZqCzjPm3da"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vD7ytsnpdO8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}